{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mpt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resto_df = pd.read_csv('ON_rest_reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resto_df = resto_df.drop(labels='Unnamed: 0', axis=1)\n",
    "resto_df = resto_df.drop(labels=['useful','funny','cool','user_id'], axis=1)\n",
    "#resto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_texts=[]\n",
    "for t in resto_df.text:\n",
    "    temp = t.replace(\"@\",\"\")\n",
    "    temp = temp.replace(\"^\",\"\")\n",
    "    temp = temp.replace(\"*\",\"\")\n",
    "    temp = temp.replace(\"{\",\"\")\n",
    "    temp = temp.replace(\"}\",\"\")\n",
    "    temp = temp.replace(\"[\",\"\")\n",
    "    temp = temp.replace(\"]\",\"\")\n",
    "    temp = temp.replace(\"|\",\"\")\n",
    "    temp = temp.replace(\"~\",\"\")\n",
    "    temp = temp.replace(\"&amp;\",\"\")\n",
    "    temp = temp.replace(\"\\n\",\"\")\n",
    "    temp = temp.replace(\"\\t\",\"\")\n",
    "    temp = temp.replace(\"\\\\x\",\"\")\n",
    "    temp = temp.replace(u'\\u200b', '')\n",
    "    temp = temp.replace(u'\\u200d', '')\n",
    "    temp = temp.replace(u'\\u201c', '')\n",
    "    temp = temp.replace(u'\\ud83e', '')\n",
    "    temp = temp.replace(u'\\udd14', '')\n",
    "    temp = temp.replace(u'\\u2019', '')\n",
    "    temp = temp.replace(\"\\a\",\"\")\n",
    "    temp = re.sub('\\\"+','',temp)\n",
    "    temp = re.sub('\\\\+','',temp)\n",
    "    temp = re.sub('http[s]?://\\S+', '', temp)\n",
    "    temp = re.sub(r'[^\\x00-\\x7F]+', '', temp)\n",
    "    review_texts.append(temp)\n",
    "    \n",
    "resto_df['text'] = review_texts\n",
    "#review_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resto_df['review_length'] = resto_df.text.map(len)\n",
    "# check how the review lengths are distributed\n",
    "#ax = sns.FacetGrid(data=resto_df, col='stars', xlim=(0, 2000)).map(mpt.hist, 'review_length', bins=50)\n",
    "#ax.axes[0][0].set(ylabel='number of reviews');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_rev = resto_df.text[resto_df.stars>3].values\n",
    "average_rev = resto_df.text[resto_df.stars==3].values\n",
    "negative_rev = resto_df.text[resto_df.stars<3].values\n",
    "#print('Postive Reviews:  {:,}'.format(len(pos_reviews)))\n",
    "#print('Negative Reviews:  {:,}'.format(len(neg_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words and Performing TF-IDF: Evaluation-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Gram Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp_wrd_list = text.ENGLISH_STOP_WORDS.union(['know', 'like','best', 've','hype' ,'got', 'just', \n",
    "               'don', 'really', 'think', 'll' , 'returning' ,'said', 'told', 'ok', 'understand' ,'great', 'poor', 'worst','pad','thai',\n",
    "                'khao', 'soi','came', 'make', 'sure', 'bad', 'dim', 'sum', 'love', 'place', 'highly', 'recommend', 'long',\n",
    "                'okay' , 'definitely', 'come', 'extremely', 'customer' , 'slow', 'minutes', 'later', 'quality', 'terrible',\n",
    "                'delicious', 'amazing', 'food', 'second', 'time', 'way', 'better', 'little', 'bit', 'excellent', 'years',\n",
    "                'friend', 'ordered', 'wasn', 'busy','horrible', 'write', 'home', 'fast', 'slow', 'mi', 'friendly',\n",
    "                'ago', 'right', 'away', 'worth', 'minute', 'minute wait', 'super', 'hit', 'miss', 'small', 'portions',\n",
    "                'multiple', 'times', 'high', 'expectations', 'expectation', 'mins',\n",
    "                'went', 'did', 'didn', 'good', 'pretty', ])\n",
    "\n",
    "pos_tfidf  = TfidfVectorizer(stop_words=stp_wrd_list, min_df=10, max_df=0.5,ngram_range=(2,2), token_pattern='[a-z][a-z]+')\n",
    "avg_tfidf  = TfidfVectorizer(stop_words=stp_wrd_list, min_df=10, max_df=0.5,ngram_range=(2,2), token_pattern='[a-z][a-z]+')\n",
    "neg_tfidf  = TfidfVectorizer(stop_words=stp_wrd_list, min_df=10, max_df=0.5,ngram_range=(2,2), token_pattern='[a-z][a-z]+')\n",
    "np.random.seed(50)\n",
    "selected_positive = np.random.choice(positive_rev,size=3000)\n",
    "selected_average = np.random.choice(average_rev,size=3000)\n",
    "selected_negative = np.random.choice(negative_rev,size=3000)\n",
    "#print(selected_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = {'absolute': 'absolutely', 'accomodate':'accommodating', 'adds': 'add', 'adding': 'add', 'agree': 'agreed',\n",
    "               'airy' : 'air', 'alcoholic':'alcohol', 'appetizers':'appetizer', 'bathrooms': 'bathroom', 'beans':'bean',\n",
    "                'beers': 'beer', 'bringing': 'bring', 'burgers': 'burger', 'burrito': 'burritos', 'cakes': 'cake',\n",
    "                'caramelized': 'caramel', 'ceilings': 'ceiling', 'cheesecakes':'cheesecake', 'chilli': 'chilly',\n",
    "                'choices': 'choice', 'cocktails': 'cocktail', 'cookies': 'cookie', 'creaminess':'creamy',\n",
    "                'crowds': 'crowded', 'desserts': 'dessert', 'drinks': 'drink', 'dumplings':'dumpling',\n",
    "                'eggs':'egg', 'flavors': 'flavor', 'fruits': 'fruit', 'games': 'game', 'grilled': 'grill',\n",
    "                'pizzas': 'pizza', 'tortillas': 'tortilla'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing common words as similar bsed on above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_selected(t_list):\n",
    "    cleaned = []\n",
    "    for t in t_list:\n",
    "        for i,j in replace_list.items():\n",
    "            t = t.replace(i,j)\n",
    "        cleaned.append(t)\n",
    "    return cleaned\n",
    "selected_positive = clean_selected(selected_positive)\n",
    "selected_average = clean_selected(selected_average)\n",
    "selected_negataive = clean_selected(selected_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wait'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wait'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wait'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "negative_vect = neg_tfidf.fit_transform(selected_negative)\n",
    "average_vect = avg_tfidf.fit_transform(selected_average)\n",
    "positive_vect = pos_tfidf.fit_transform(selected_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df = pd.DataFrame(negative_vect.todense(), columns=[neg_tfidf.get_feature_names()])\n",
    "average_df = pd.DataFrame(average_vect.todense(), columns=[avg_tfidf.get_feature_names()])\n",
    "positive_df = pd.DataFrame(positive_vect.todense(), columns=[pos_tfidf.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_negative = {}\n",
    "mean_average = {}\n",
    "mean_positive = {}\n",
    "\n",
    "for col_index in negative_df:\n",
    "    mean_negative[col_index] = negative_df[col_index].mean()\n",
    "\n",
    "for col_index in average_df:\n",
    "    mean_average[col_index] = average_df[col_index].mean()\n",
    "    \n",
    "for col_index in positive_df:\n",
    "    mean_positive[col_index] = positive_df[col_index].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "[('ice cream',), ('wait staff',), ('fried chicken',), ('butter chicken',), ('deep fried',), ('overall experience',), ('save money',), ('mac cheese',), ('dining experience',), ('pulled pork',)]\n",
      "Avg\n",
      "[('ice cream',), ('pork belly',), ('fried chicken',), ('green tea',), ('deep fried',), ('mac cheese',), ('portion size',), ('overall experience',), ('friday night',), ('late night',)]\n",
      "Positive\n",
      "[('ice cream',), ('pork belly',), ('fried chicken',), ('deep fried',), ('pulled pork',), ('green tea',), ('service quick',), ('fish tacos',), ('fried rice',), ('chicken waffles',)]\n"
     ]
    }
   ],
   "source": [
    "print('Negative')\n",
    "print(sorted(mean_negative, key=mean_negative.get, reverse=True)[:10])\n",
    "\n",
    "print('Avg')\n",
    "print(sorted(mean_average, key=mean_average.get, reverse=True)[:10])\n",
    "\n",
    "print('Positive')\n",
    "print(sorted(mean_positive, key=mean_positive.get, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### It can be seen that in all 3 categories of TF-IDF, the majority phrases are related to service and order. \n",
    "### Hence only using TF-IDF, not much business insights can be obtained and topic modelling is required.\n",
    "### We will use the vectors generated by TF-IDF for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del negative_df\n",
    "del positive_df\n",
    "del average_df\n",
    "del resto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For genism we need terms by docs, so taking transpose of 3 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_corpus = matutils.Sparse2Corpus(positive_vect.transpose())\n",
    "average_corpus = matutils.Sparse2Corpus(average_vect.transpose())\n",
    "negative_corpus = matutils.Sparse2Corpus(negative_vect.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interchanging key, value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row indices\n",
    "positive_id2 = dict((v,k) for k,v in pos_tfidf.vocabulary_.items())\n",
    "average_id2 = dict((v,k) for k,v in pos_tfidf.vocabulary_.items())\n",
    "negative_id2 = dict((v,k) for k,v in pos_tfidf.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_id2 = corpora.Dictionary.from_corpus(positive_corpus, id2word=positive_id2)\n",
    "average_id2 = corpora.Dictionary.from_corpus(average_corpus, id2word=average_id2)\n",
    "negative_id2 = corpora.Dictionary.from_corpus(negative_corpus, id2word=negative_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_topics_lsi = models.lsimulticore.LsiMulticore(positive_corpus, id2word=positive_id2, num_topics=5, passes=10, workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_topics_lsi = models.LsiModel(average_corpus, id2word=average_id2, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative_topics_lsi = models.LsiModel(negative_corpus, id2word=negative_id2, num_topics=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wait'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wait'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wait'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "positive_cnt_vect = CountVectorizer(stop_words=stp_wrd_list, min_df=10, max_df=0.5,ngram_range=(2,2), token_pattern='[a-z][a-z]+')\n",
    "average_cnt_vect = CountVectorizer(stop_words=stp_wrd_list, min_df=10, max_df=0.5,ngram_range=(2,2), token_pattern='[a-z][a-z]+')\n",
    "negative_cnt_vect = CountVectorizer(stop_words=stp_wrd_list, min_df=10, max_df=0.5,ngram_range=(2,2), token_pattern='[a-z][a-z]+')\n",
    "\n",
    "np.random.seed(50)\n",
    "cnt_selected_negative = np.random.choice(negative_rev, size=10000)\n",
    "cnt_selected_positive = np.random.choice(positive_rev, size=10000)\n",
    "cnt_selected_average = np.random.choice(average_rev, size=10000)\n",
    "\n",
    "cnt_selected_negative = clean_selected(cnt_selected_negative)\n",
    "cnt_selected_positive = clean_selected(cnt_selected_positive)\n",
    "cnt_selected_average = clean_selected(cnt_selected_average)\n",
    "\n",
    "cnt_negative_vect = negative_cnt_vect.fit_transform(cnt_selected_negative).transpose()\n",
    "cnt_positive_vect = positive_cnt_vect.fit_transform(cnt_selected_positive).transpose()\n",
    "cnt_average_vect = average_cnt_vect.fit_transform(cnt_selected_average).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "negative_corp = matutils.Sparse2Corpus(cnt_negative_vect)\n",
    "average_corp = matutils.Sparse2Corpus(cnt_average_vect)\n",
    "positive_corp = matutils.Sparse2Corpus(cnt_positive_vect)\n",
    "\n",
    "id2_pos = dict((v, k) for k, v in positive_cnt_vect.vocabulary_.items())\n",
    "id2_avg = dict((v, k) for k, v in average_cnt_vect.vocabulary_.items())\n",
    "id2_neg = dict((v, k) for k, v in negative_cnt_vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "positive_lda = models.ldamulticore.LdaMulticore(corpus=positive_corp, num_topics=5,id2word=id2_pos, passes=10, workers=15)\n",
    "average_lda = models.ldamulticore.LdaMulticore(corpus=average_corp, num_topics=5,id2word=id2_avg, passes=10, workers=15)\n",
    "negative_lda = models.ldamulticore.LdaMulticore(corpus=negative_corp, num_topics=5,id2word=id2_neg, passes=10, workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : \n",
      "[(0,\n",
      "  '0.014*\"fried chicken\" + 0.009*\"hot sauce\" + 0.009*\"fish chips\" + '\n",
      "  '0.007*\"baja fish\" + 0.006*\"chicken katsu\"'),\n",
      " (1,\n",
      "  '0.013*\"pork belly\" + 0.013*\"pulled pork\" + 0.010*\"green curry\" + '\n",
      "  '0.008*\"spring rolls\" + 0.008*\"sweet potato\"'),\n",
      " (2,\n",
      "  '0.013*\"pork belly\" + 0.010*\"mac cheese\" + 0.008*\"fried chicken\" + '\n",
      "  '0.006*\"bone marrow\" + 0.006*\"smoked meat\"'),\n",
      " (3,\n",
      "  '0.031*\"ice cream\" + 0.014*\"green tea\" + 0.011*\"deep fried\" + 0.008*\"duck '\n",
      "  'fat\" + 0.008*\"late night\"'),\n",
      " (4,\n",
      "  '0.011*\"french toast\" + 0.009*\"chicken waffles\" + 0.008*\"iced tea\" + '\n",
      "  '0.008*\"kimchi fries\" + 0.007*\"egg benny\"')]\n",
      "Average : \n",
      "[(0,\n",
      "  '0.018*\"pork belly\" + 0.005*\"ramen places\" + 0.005*\"san road\" + 0.005*\"duck '\n",
      "  'fat\" + 0.004*\"soon tofu\"'),\n",
      " (1,\n",
      "  '0.007*\"smoked meat\" + 0.005*\"uncle tetsu\" + 0.005*\"cheese cake\" + '\n",
      "  '0.005*\"beef brisket\" + 0.004*\"quite nice\"'),\n",
      " (2,\n",
      "  '0.011*\"deep fried\" + 0.006*\"ice cream\" + 0.006*\"late night\" + 0.006*\"fried '\n",
      "  'chicken\" + 0.005*\"hot sauce\"'),\n",
      " (3,\n",
      "  '0.018*\"ice cream\" + 0.009*\"soft serve\" + 0.009*\"green tea\" + 0.008*\"sweet '\n",
      "  'potato\" + 0.006*\"red bean\"'),\n",
      " (4,\n",
      "  '0.009*\"cheese tart\" + 0.009*\"fried chicken\" + 0.007*\"butter chicken\" + '\n",
      "  '0.007*\"french toast\" + 0.006*\"spring rolls\"')]\n",
      "Negative : \n",
      "[(0,\n",
      "  '0.006*\"pork belly\" + 0.004*\"green tea\" + 0.004*\"soft serve\" + 0.004*\"olive '\n",
      "  'oil\" + 0.004*\"asked wanted\"'),\n",
      " (1,\n",
      "  '0.012*\"ice cream\" + 0.008*\"mac cheese\" + 0.007*\"half hour\" + 0.007*\"deep '\n",
      "  'fried\" + 0.006*\"fried chicken\"'),\n",
      " (2,\n",
      "  '0.010*\"pulled pork\" + 0.005*\"butter chicken\" + 0.005*\"cheese tart\" + '\n",
      "  '0.005*\"pork buns\" + 0.005*\"hour wait\"'),\n",
      " (3,\n",
      "  '0.005*\"spring rolls\" + 0.005*\"smoked meat\" + 0.004*\"egg benedict\" + '\n",
      "  '0.004*\"service server\" + 0.004*\"torched sushi\"'),\n",
      " (4,\n",
      "  '0.008*\"ice cream\" + 0.006*\"gluten free\" + 0.006*\"noodle bar\" + '\n",
      "  '0.004*\"burger priest\" + 0.004*\"sweet sour\"')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Positive : \")\n",
    "pprint(positive_lda.print_topics(num_words=5))\n",
    "\n",
    "print(\"Average : \")\n",
    "pprint(average_lda.print_topics(num_words=5))\n",
    "\n",
    "print(\"Negative : \")\n",
    "pprint(negative_lda.print_topics(num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 349 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "positive_nmf = NMF(n_components=5)\n",
    "\n",
    "# for NMF, we know that A =WH where w is basis vector nd h is co-efficient vector\n",
    "\n",
    "positive_W_Mat = positive_nmf.fit_transform(positive_vect)\n",
    "positive_H_Mat = positive_nmf.components_\n",
    "\n",
    "average_nmf = NMF(n_components=5)\n",
    "average_W_Mat = average_nmf.fit_transform(average_vect)\n",
    "average_H_Mat = average_nmf.components_\n",
    "\n",
    "negative_nmf = NMF(n_components=5)\n",
    "negative_W_Mat = negative_nmf.fit_transform(negative_vect)\n",
    "negative_H_Mat = negative_nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: -------------\n",
      "Topic 0:   2.370*ice cream, 0.776*green tea, 0.388*black sesame, 0.329*soft serve, 0.149*whipped cream\n",
      "Topic 1:   2.386*pork belly, 0.339*kimchi fries, 0.233*belly banh, 0.201*banh boys, 0.112*sweet potato\n",
      "Topic 2:   1.917*fried chicken, 0.553*chicken waffles, 0.129*wait line, 0.121*french toast, 0.106*beef brisket\n",
      "Topic 3:   1.895*deep fried, 0.396*big fan, 0.252*fish tacos, 0.183*fried rice, 0.167*hot sauce\n",
      "Topic 4:   1.874*pulled pork, 0.493*pork sandwich, 0.248*bbq sauce, 0.226*service quick, 0.218*mac cheese\n",
      "Average: -------------\n",
      "Topic 0:   2.496*ice cream, 0.237*soft serve, 0.111*serve ice, 0.108*waitress nice, 0.096*tea ice\n",
      "Topic 1:   2.391*fried chicken, 0.182*chicken thigh, 0.155*deep fried, 0.118*sweet liking, 0.106*fried rice\n",
      "Topic 2:   1.873*pork belly, 0.267*kimchi fries, 0.223*soup base, 0.181*duck confit, 0.150*ramen places\n",
      "Topic 3:   1.530*mac cheese, 0.490*deep fried, 0.477*late night, 0.406*sweet potato, 0.273*fish tacos\n",
      "Topic 4:   1.890*green tea, 0.373*soft serve, 0.322*red bean, 0.190*tea ice, 0.173*uncle tetsu\n",
      "Negative: -------------\n",
      "Topic 0:   2.269*ice cream, 0.177*soft serve, 0.130*green tea, 0.102*olive oil, 0.099*service meh\n",
      "Topic 1:   2.355*wait staff, 0.317*wait line, 0.182*french fries, 0.127*bar wait, 0.114*dining experience\n",
      "Topic 2:   1.973*butter chicken, 0.187*mac cheese, 0.151*eggs benedict, 0.143*wanted order, 0.097*tasted bland\n",
      "Topic 3:   1.911*fried chicken, 0.466*pulled pork, 0.198*mac cheese, 0.186*chicken wings, 0.171*spend money\n",
      "Topic 4:   1.392*deep fried, 0.365*overall experience, 0.350*save money, 0.259*service server, 0.226*looking forward\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, num_topics, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx < num_topics:\n",
    "            print(\"{:11}\".format(\"Topic %d:\" %(topic_idx)), end='')\n",
    "            print(\", \".join(['{:04.3f}*'.format(topic[i])+feature_names[i] \\\n",
    "                             for i in topic.argsort()[:-no_top_words-1:-1]]))\n",
    "            \n",
    "no_topics = 5\n",
    "no_top_words = 5\n",
    "\n",
    "print(\"Positive: -------------\")\n",
    "display_topics(positive_nmf, pos_tfidf.get_feature_names(), no_topics, no_top_words)\n",
    "\n",
    "print(\"Average: -------------\")\n",
    "display_topics(average_nmf, avg_tfidf.get_feature_names(), no_topics, no_top_words)\n",
    "\n",
    "print(\"Negative: -------------\")\n",
    "display_topics(negative_nmf, neg_tfidf.get_feature_names(), no_topics, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a random positive review and anlysing it\n",
    "def rev_analysis(sel_revs, W_Mat):\n",
    "    rand_num = np.random.randint(0, high=len(sel_revs))\n",
    "    print('Review Number: ',rand_num, '-------------')\n",
    "    print(sel_revs[rand_num])\n",
    "    print(\"------------\")\n",
    "    top = {}\n",
    "    for index, j in enumerate(W_Mat[rand_num]):\n",
    "        top[index] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Number:  456 -------------\n",
      "4.5 StarsAfter hearing tons of people talking about Smash, I finally went to try!Upon entering, it was pretty empty since I came at the awkward 3pm which is right after the lunch rush. You are greeted by some friendly waitresses and left to gawk at the awesome interior of the place. It's classy and casual at the same time. I feel like you could walk in with jeans or a little black dress. It reminded me of moxies, but with a classier and more upscale design.Now onto the food....Since I came with my friend who was starving, we mostly shared a few dishes, mostly appetizer. Let me start with the truffle mac and cheese though. This is the bomb. I usually don't indulge in mac and cheese but...... this was so creamy and decadent. The truffle definitely upgraded it to another level. The pasta that they used was shaped like a fancy swirl. It was served on a skillet for aesthetic purposes (and to keep it hot). This was definitely a highlight, but not for the fainthearted, i could feel my arteries clogging.The Calamari appetizer was quite good as well, the batter was light buttermilk and really complimented the dish, there were two sauces that were served with it. One was creamy and the other was more of a tomato sauce. Both were delish!The Guac & Chips were good but nothing to brag about. price was a bit steep, but the guac was a LOT that I ended up taking some home. the guac was very fresh and this would be a good healthy optionLastly, The Tomato and Corn Soup was good, but I'm not the biggest fan of thick soups. I like more of a brothy consistency but the flavour was blasting in the soup and was great with some bread dipped in it. Overall, the food was quite good and my bill came in just short of 60 dollars. The waitresses are very witty and made the experience very comfortable. I would definitely be back to try out their pizza and other mac & cheese ( they have a lobster one that i'm eyeing rn)\n",
      "------------\n",
      "Topic 4: 0.0472\n",
      "Topic 3: 0.0225\n",
      "Topic 0: 0.0025\n",
      "Topic 1: 0.0000\n",
      "Topic 2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "rev_analysis(selected_positive, positive_W_Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Number:  413 -------------\n",
      "Don't see what the big fuss is about Wilbur, my second time here, had the burritos the first time (per my husband's recommendation). I would had given that a 3-star as well. My friend really liked Wilbur and so we met here again and this time I got the Shrimp Fajitas, there was no pepper next to any menu items to indicate that it was spicy. I doubt I'm the only one who wish they would include that. Their pineapple salsa from the bar is my favourite. Their line moves pretty quickly for ordering. It's waiting for the food that could take about 15 minutes in the heart of lunch rush.\n",
      "------------\n",
      "Topic 3: 0.0161\n",
      "Topic 4: 0.0136\n",
      "Topic 1: 0.0037\n",
      "Topic 2: 0.0018\n",
      "Topic 0: 0.0000\n"
     ]
    }
   ],
   "source": [
    "rev_analysis(selected_average, average_W_Mat)\n",
    "#len(average_W_Mat)\n",
    "#negative_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Number:  1299 -------------\n",
      "Disappointed. How do they claim to be an Italian restaurant when their pasta is worse than East Side Mario's?Spaghetti in Canna a Mare (fresh clams and mussels, calamari, scallops and tiger shrimp, light tomato sauce, mildly spicy) for $22.95 - Let me start off by saying that this pasta does not come with cheese :( but that's not even the problem. The pasta itself is SEVERELY UNDERCOOKED. I might as well have eaten the pasta raw and dry. It was extremely stiff and tough to chew on, my jaw was aching after 2 bites. The seafood was also undercooked, the mussels weren't even opened yet. Overall, it was bland. Very bland. There was no flavour to it whatsoever. It's not even a light pasta, it just had no seasoning whatsoever and it really lacked a sauce. I just ate all the raw seafood and left the pasta because I'm pretty sure I wouldn't be able to digest it. Seafood pasta is my all-time favourite pasta dish and Terroni has ruined it for me.Tortelloni di Ricotta (handmade spinach tortelloni filled with ricotta and pecorino with butter, sage and parmigiano) for $22.95 - this pasta was very dry but the filling of the tortelloni itself was yummy. Flavours were much better than the seafood pasta, but the mushrooms were very dehydrated and had this weird chewy texture to it. Carpaccio di Manzo (hand cut, raw beef tenderloin, lemon juice, extra virgin olive oil, parmigiano reggiano shavings, arugola) for $15.95 - I am aware that carpaccio is thin, but this was so thin I couldn't even pick it off the plate to eat, making it very difficult to even enjoy this dish. The flavour combination was okay there was pretty much no tenderloin.\n",
      "------------\n",
      "Topic 4: 0.0544\n",
      "Topic 0: 0.0067\n",
      "Topic 2: 0.0016\n",
      "Topic 1: 0.0000\n",
      "Topic 3: 0.0000\n"
     ]
    }
   ],
   "source": [
    "rev_analysis(selected_negative, negative_W_Mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's perform modelling to all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "positive_rev = clean_selected(positive_rev)\n",
    "average_rev = clean_selected(average_rev)\n",
    "negataive_rev = clean_selected(negative_rev)\n",
    "\n",
    "positive_vect = pos_tfidf.fit_transform(positive_rev)\n",
    "average_vect = avg_tfidf.fit_transform(average_rev)\n",
    "negative_vect = neg_tfidf.fit_transform(negative_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: -------------\n",
      "Topic 0:   3.186*pork belly, 1.682*kimchi fries, 0.736*spice pork, 0.517*banh boys, 0.476*belly banh\n",
      "Topic 1:   2.283*ice cream, 0.897*green tea, 0.510*soft serve, 0.462*black sesame, 0.295*tea ice\n",
      "Topic 2:   2.231*fried chicken, 0.555*chicken waffles, 0.207*deep fried, 0.179*chicken bao, 0.141*chicken dinner\n",
      "Topic 3:   0.875*green curry, 0.471*duck fat, 0.462*fish tacos, 0.457*french toast, 0.431*fish chips\n",
      "Topic 4:   2.213*pulled pork, 0.621*pork sandwich, 0.308*beef brisket, 0.274*mac cheese, 0.271*bbq sauce\n",
      "Average: -------------\n",
      "Topic 0:   2.759*ice cream, 0.639*soft serve, 0.249*serve ice, 0.226*tea ice, 0.174*egg waffles\n",
      "Topic 1:   2.575*pork belly, 0.650*kimchi fries, 0.220*ramen places, 0.218*spice pork, 0.217*pulled pork\n",
      "Topic 2:   0.630*mac cheese, 0.604*fish tacos, 0.525*late night, 0.373*french toast, 0.321*deep fried\n",
      "Topic 3:   2.169*fried chicken, 0.285*chicken waffles, 0.235*deep fried, 0.141*chicken dinner, 0.117*pulled pork\n",
      "Topic 4:   2.118*green tea, 0.339*red bean, 0.332*tea ice, 0.242*uncle tetsu, 0.190*tea cheesecake\n",
      "Negative: -------------\n",
      "Topic 0:   2.584*ice cream, 0.367*soft serve, 0.363*green tea, 0.188*egg waffle, 0.176*tea ice\n",
      "Topic 1:   0.830*wait staff, 0.559*half hour, 0.419*waste money, 0.377*waited hour, 0.374*took forever\n",
      "Topic 2:   2.030*deep fried, 0.139*fish chips, 0.107*burger priest, 0.095*spring rolls, 0.090*fried squid\n",
      "Topic 3:   2.032*butter chicken, 0.328*chicken sweet, 0.178*palak paneer, 0.167*chicken tikka, 0.132*chicken tasted\n",
      "Topic 4:   1.730*fried chicken, 0.332*smoked meat, 0.303*mac cheese, 0.240*pork belly, 0.224*pulled pork\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive: -------------\")\n",
    "display_topics(positive_nmf, pos_tfidf.get_feature_names(), no_topics, no_top_words)\n",
    "\n",
    "print(\"Average: -------------\")\n",
    "display_topics(average_nmf, avg_tfidf.get_feature_names(), no_topics, no_top_words)\n",
    "\n",
    "print(\"Negative: -------------\")\n",
    "display_topics(negative_nmf, neg_tfidf.get_feature_names(), no_topics, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming topics, to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {0:'pork belly/kimchi fries', 1:'ice cream/green tea', 2:'fried chicken/ chicken waffles', \n",
    "              3:'green curry/ duck fat/ fish tacos', 4:'pork sandwich/ beef brisket/mac cheese'}\n",
    "\n",
    "avg_dict = {0:'ice cream/egg waffles', 1:'pork belly/kimchi fries/ ramen places', 2:'mac cheese/ fish tacos', \n",
    "              3:'fried chicken/ deep fried', 4:'green tea/ red bean/cheesecake'}\n",
    "\n",
    "neg_dict = {0:'ice cream/ service', 1:'waiting/ staff', 2:'deep fried/fish chips/ spirng rolls', \n",
    "              3:'butter chicken/ palak paneer', 4:'fried chicken/ smoked meat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_rev_df = resto_df[['business_id', 'stars', 'text', 'review_length']][resto_df.stars>3].reset_index()\n",
    "average_rev_df = resto_df[['business_id', 'stars', 'text', 'review_length']][resto_df.stars==3].reset_index()\n",
    "negative_rev_df = resto_df[['business_id', 'stars', 'text', 'review_length']][resto_df.stars<3].reset_index()\n",
    "positive_rev_df.drop(labels='index', axis=1, inplace=True)\n",
    "average_rev_df.drop(labels='index', axis=1, inplace=True)\n",
    "negative_rev_df.drop(labels='index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the weights so they make 1 and are more readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_rev_df = pd.concat([positive_rev_df, pd.DataFrame(normalize(positive_W_Mat, norm='l1'))], axis=1)\n",
    "average_rev_df = pd.concat([average_rev_df, pd.DataFrame(normalize(average_W_Mat, norm='l1'))], axis=1)\n",
    "negative_rev_df = pd.concat([negative_rev_df, pd.DataFrame(normalize(negative_W_Mat, norm='l1'))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_mean_pos = positive_rev_df.groupby('business_id').agg({'stars':'mean', 'text':'count','review_length':'mean',0:'mean', \n",
    "                                                1:'mean', 2:'mean', 3:'mean',4:'mean'}).reset_index()\n",
    "grp_mean_avg = average_rev_df.groupby('business_id').agg({'stars':'mean', 'text':'count','review_length':'mean',0:'mean', \n",
    "                                                 1:'mean', 2:'mean', 3:'mean',4:'mean'}).reset_index()\n",
    "grp_mean_neg = negative_rev_df.groupby('business_id').agg({'stars':'mean', 'text':'count','review_length':'mean',0:'mean', \n",
    "                                                 1:'mean', 2:'mean', 3:'mean',4:'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_mean_pos.drop(labels='text', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_mean_avg.drop(labels='text', axis=1, inplace=True)\n",
    "grp_mean_neg.drop(labels='text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_length</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-av1lZI1JDY_RZN2eTMnWg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>642.062222</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.878158</td>\n",
       "      <td>0.016790</td>\n",
       "      <td>0.007879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0J_NiF5Lb0bFM5v6ZY-uGw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>647.870370</td>\n",
       "      <td>0.017626</td>\n",
       "      <td>0.022931</td>\n",
       "      <td>0.836254</td>\n",
       "      <td>0.061339</td>\n",
       "      <td>0.024813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0a2O150ytxrDjDzXNfRWkA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>802.539474</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>0.026676</td>\n",
       "      <td>0.603201</td>\n",
       "      <td>0.021654</td>\n",
       "      <td>0.238362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0v8icS8wOOgEDiHDCOQkZQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>532.789474</td>\n",
       "      <td>0.023698</td>\n",
       "      <td>0.039304</td>\n",
       "      <td>0.677592</td>\n",
       "      <td>0.066322</td>\n",
       "      <td>0.035189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2Q89PhkcyT9uZnQORmqMKg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>658.157143</td>\n",
       "      <td>0.026352</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.827557</td>\n",
       "      <td>0.054777</td>\n",
       "      <td>0.028061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>yY3jNsrpCyKTqQuRuLV8gw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1044.584615</td>\n",
       "      <td>0.021610</td>\n",
       "      <td>0.022739</td>\n",
       "      <td>0.873639</td>\n",
       "      <td>0.053528</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>yg_A_TpYkJjr1fef0J6QkQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>887.258824</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.443021</td>\n",
       "      <td>0.334613</td>\n",
       "      <td>0.095143</td>\n",
       "      <td>0.069981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>zA6gnF5aPBGoOm6uIbKt-A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>678.637681</td>\n",
       "      <td>0.059825</td>\n",
       "      <td>0.026937</td>\n",
       "      <td>0.726528</td>\n",
       "      <td>0.028232</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>zfQ855VX3SMA_54oVSN5Cw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>588.117647</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.860868</td>\n",
       "      <td>0.028444</td>\n",
       "      <td>0.032054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>zgQHtqX0gqMw1nlBZl2VnQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>857.947955</td>\n",
       "      <td>0.127716</td>\n",
       "      <td>0.391434</td>\n",
       "      <td>0.306023</td>\n",
       "      <td>0.126121</td>\n",
       "      <td>0.026401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id  stars  review_length         0         1  \\\n",
       "0    -av1lZI1JDY_RZN2eTMnWg    3.0     642.062222  0.019575  0.024264   \n",
       "1    0J_NiF5Lb0bFM5v6ZY-uGw    3.0     647.870370  0.017626  0.022931   \n",
       "2    0a2O150ytxrDjDzXNfRWkA    3.0     802.539474  0.044317  0.026676   \n",
       "3    0v8icS8wOOgEDiHDCOQkZQ    3.0     532.789474  0.023698  0.039304   \n",
       "4    2Q89PhkcyT9uZnQORmqMKg    3.0     658.157143  0.026352  0.034682   \n",
       "..                      ...    ...            ...       ...       ...   \n",
       "166  yY3jNsrpCyKTqQuRuLV8gw    3.0    1044.584615  0.021610  0.022739   \n",
       "167  yg_A_TpYkJjr1fef0J6QkQ    3.0     887.258824  0.057241  0.443021   \n",
       "168  zA6gnF5aPBGoOm6uIbKt-A    3.0     678.637681  0.059825  0.026937   \n",
       "169  zfQ855VX3SMA_54oVSN5Cw    3.0     588.117647  0.010099  0.009711   \n",
       "170  zgQHtqX0gqMw1nlBZl2VnQ    3.0     857.947955  0.127716  0.391434   \n",
       "\n",
       "            2         3         4  \n",
       "0    0.878158  0.016790  0.007879  \n",
       "1    0.836254  0.061339  0.024813  \n",
       "2    0.603201  0.021654  0.238362  \n",
       "3    0.677592  0.066322  0.035189  \n",
       "4    0.827557  0.054777  0.028061  \n",
       "..        ...       ...       ...  \n",
       "166  0.873639  0.053528  0.013100  \n",
       "167  0.334613  0.095143  0.069981  \n",
       "168  0.726528  0.028232  0.115000  \n",
       "169  0.860868  0.028444  0.032054  \n",
       "170  0.306023  0.126121  0.026401  \n",
       "\n",
       "[171 rows x 8 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_mean_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending the restaurant details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_resto = pd.read_csv('Ontario_resto.csv')\n",
    "on_resto.drop(labels='Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>c35qzLN6ItDpVIoj2uQW4Q</td>\n",
       "      <td>Ravi Soups</td>\n",
       "      <td>322 Adelaide Street W</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5V 1R1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>332</td>\n",
       "      <td>{'Alcohol': \"u'beer_and_wine'\", 'BikeParking':...</td>\n",
       "      <td>Soup, Restaurants, Sandwiches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>uF86ZhygpBEGr3CudNemYA</td>\n",
       "      <td>O.Noir</td>\n",
       "      <td>620 Church St</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M4Y 2G2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>301</td>\n",
       "      <td>{'GoodForMeal': \"{'dessert': False, 'latenight...</td>\n",
       "      <td>Canadian (New), American (New), Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>XYIPXJ9parr9FtvvcGI1SA</td>\n",
       "      <td>GB Hand-pulled Noodles</td>\n",
       "      <td>66 Edward Street</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5G 1C9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>421</td>\n",
       "      <td>{'RestaurantsDelivery': 'False', 'HasTV': 'Fal...</td>\n",
       "      <td>Chinese, Restaurants, Noodles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>WKOUTdVJS58E178JjhwidQ</td>\n",
       "      <td>Pastel Creperies &amp; Dessert House</td>\n",
       "      <td>5417 Yonge Street</td>\n",
       "      <td>North York</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N 5R6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>316</td>\n",
       "      <td>{'RestaurantsDelivery': 'False', 'GoodForMeal'...</td>\n",
       "      <td>Food, Restaurants, Coffee &amp; Tea, Creperies, De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>O-uIEuv7JLUHajkemx_sVw</td>\n",
       "      <td>Beerbistro</td>\n",
       "      <td>18 King Street E</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5C 1C4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>596</td>\n",
       "      <td>{'RestaurantsReservations': 'True', 'Restauran...</td>\n",
       "      <td>Restaurants, Bistros, Bars, Beer Bar, Pubs, Ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>A7waf6G3cvnLfAqKeLL8DA</td>\n",
       "      <td>Buca</td>\n",
       "      <td>604 King Street W</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5V 1M6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>392</td>\n",
       "      <td>{'RestaurantsAttire': \"u'dressy'\", 'Caters': '...</td>\n",
       "      <td>Restaurants, Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>O66Zy8Y13VBm72ZDhS4fIg</td>\n",
       "      <td>Sassafraz</td>\n",
       "      <td>100 Cumberland Street</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5R 1A6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>325</td>\n",
       "      <td>{'BikeParking': 'True', 'RestaurantsPriceRange...</td>\n",
       "      <td>American (New), Restaurants, Event Planning &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>XCxxPZ3Lu5mwmIo7IQRf1g</td>\n",
       "      <td>Osaka Sushi Japanese Korean Restaurant</td>\n",
       "      <td>5762 Highway 7 E</td>\n",
       "      <td>Markham</td>\n",
       "      <td>ON</td>\n",
       "      <td>L3P 1A8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>323</td>\n",
       "      <td>{'RestaurantsGoodForGroups': 'True', 'Restaura...</td>\n",
       "      <td>Korean, Sushi Bars, Restaurants, Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>5aeOewSy4RiI8sLLWpeNGA</td>\n",
       "      <td>Pablo Cheesetart Canada</td>\n",
       "      <td>114 Dundas St W</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5G 1C3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>308</td>\n",
       "      <td>{'RestaurantsPriceRange2': '2', 'RestaurantsDe...</td>\n",
       "      <td>Desserts, Food, Restaurants, Cafes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>k6zmSLmYAquCpJGKNnTgSQ</td>\n",
       "      <td>The Stockyards</td>\n",
       "      <td>699 Saint Clair Avenue W</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M6C 1B2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>693</td>\n",
       "      <td>{'BusinessParking': \"{'garage': False, 'street...</td>\n",
       "      <td>American (Traditional), Barbeque, Restaurants,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id                                    name  \\\n",
       "0    c35qzLN6ItDpVIoj2uQW4Q                              Ravi Soups   \n",
       "1    uF86ZhygpBEGr3CudNemYA                                  O.Noir   \n",
       "2    XYIPXJ9parr9FtvvcGI1SA                  GB Hand-pulled Noodles   \n",
       "3    WKOUTdVJS58E178JjhwidQ        Pastel Creperies & Dessert House   \n",
       "4    O-uIEuv7JLUHajkemx_sVw                              Beerbistro   \n",
       "..                      ...                                     ...   \n",
       "166  A7waf6G3cvnLfAqKeLL8DA                                    Buca   \n",
       "167  O66Zy8Y13VBm72ZDhS4fIg                               Sassafraz   \n",
       "168  XCxxPZ3Lu5mwmIo7IQRf1g  Osaka Sushi Japanese Korean Restaurant   \n",
       "169  5aeOewSy4RiI8sLLWpeNGA                 Pablo Cheesetart Canada   \n",
       "170  k6zmSLmYAquCpJGKNnTgSQ                          The Stockyards   \n",
       "\n",
       "                      address        city state postal_code  stars  \\\n",
       "0       322 Adelaide Street W     Toronto    ON     M5V 1R1    4.5   \n",
       "1               620 Church St     Toronto    ON     M4Y 2G2    3.5   \n",
       "2            66 Edward Street     Toronto    ON     M5G 1C9    4.0   \n",
       "3           5417 Yonge Street  North York    ON     M2N 5R6    4.0   \n",
       "4            18 King Street E     Toronto    ON     M5C 1C4    4.0   \n",
       "..                        ...         ...   ...         ...    ...   \n",
       "166         604 King Street W     Toronto    ON     M5V 1M6    4.0   \n",
       "167     100 Cumberland Street     Toronto    ON     M5R 1A6    3.5   \n",
       "168          5762 Highway 7 E     Markham    ON     L3P 1A8    4.0   \n",
       "169           114 Dundas St W     Toronto    ON     M5G 1C3    3.0   \n",
       "170  699 Saint Clair Avenue W     Toronto    ON     M6C 1B2    4.0   \n",
       "\n",
       "     review_count                                         attributes  \\\n",
       "0             332  {'Alcohol': \"u'beer_and_wine'\", 'BikeParking':...   \n",
       "1             301  {'GoodForMeal': \"{'dessert': False, 'latenight...   \n",
       "2             421  {'RestaurantsDelivery': 'False', 'HasTV': 'Fal...   \n",
       "3             316  {'RestaurantsDelivery': 'False', 'GoodForMeal'...   \n",
       "4             596  {'RestaurantsReservations': 'True', 'Restauran...   \n",
       "..            ...                                                ...   \n",
       "166           392  {'RestaurantsAttire': \"u'dressy'\", 'Caters': '...   \n",
       "167           325  {'BikeParking': 'True', 'RestaurantsPriceRange...   \n",
       "168           323  {'RestaurantsGoodForGroups': 'True', 'Restaura...   \n",
       "169           308  {'RestaurantsPriceRange2': '2', 'RestaurantsDe...   \n",
       "170           693  {'BusinessParking': \"{'garage': False, 'street...   \n",
       "\n",
       "                                            categories  \n",
       "0                        Soup, Restaurants, Sandwiches  \n",
       "1          Canadian (New), American (New), Restaurants  \n",
       "2                        Chinese, Restaurants, Noodles  \n",
       "3    Food, Restaurants, Coffee & Tea, Creperies, De...  \n",
       "4    Restaurants, Bistros, Bars, Beer Bar, Pubs, Ni...  \n",
       "..                                                 ...  \n",
       "166                               Restaurants, Italian  \n",
       "167  American (New), Restaurants, Event Planning & ...  \n",
       "168          Korean, Sushi Bars, Restaurants, Japanese  \n",
       "169                 Desserts, Food, Restaurants, Cafes  \n",
       "170  American (Traditional), Barbeque, Restaurants,...  \n",
       "\n",
       "[171 rows x 10 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_resto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pos = grp_mean_pos.merge(on_resto[['business_id', 'name', 'city', 'stars', 'review_count', 'address', 'postal_code']], left_on='business_id',right_on='business_id')[['business_id', 'name', 'address', 'postal_code' , 'city', 'stars_y','review_count',0, 1, 2, 3, 4]]\n",
    "\n",
    "final_avg = grp_mean_avg.merge(on_resto[['business_id', 'name', 'city', 'stars', 'review_count', 'address', 'postal_code']], left_on='business_id', right_on='business_id')[['business_id', 'name', 'address', 'postal_code', 'city', 'stars_y','review_count', 0, 1, 2, 3, 4]]\n",
    "\n",
    "final_neg = grp_mean_neg.merge(on_resto[['business_id', 'name', 'city', 'stars', 'review_count', 'address', 'postal_code']], left_on='business_id', right_on='business_id')[['business_id', 'name', 'address', 'postal_code', 'city', 'stars_y','review_count', 0, 1, 2, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>city</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-av1lZI1JDY_RZN2eTMnWg</td>\n",
       "      <td>Salad King Restaurant</td>\n",
       "      <td>340 Yonge Street</td>\n",
       "      <td>M5B 1R8</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>3.5</td>\n",
       "      <td>876</td>\n",
       "      <td>0.021768</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>0.018437</td>\n",
       "      <td>0.910842</td>\n",
       "      <td>0.009212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0J_NiF5Lb0bFM5v6ZY-uGw</td>\n",
       "      <td>Zet's Restaurant</td>\n",
       "      <td>6445 Airport Road</td>\n",
       "      <td>L4V 1Y4</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>4.0</td>\n",
       "      <td>333</td>\n",
       "      <td>0.027666</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.052672</td>\n",
       "      <td>0.733037</td>\n",
       "      <td>0.111818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0a2O150ytxrDjDzXNfRWkA</td>\n",
       "      <td>Miku</td>\n",
       "      <td>105-10 Bay Street</td>\n",
       "      <td>M5J 2R8</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>4.0</td>\n",
       "      <td>604</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>0.343642</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.606169</td>\n",
       "      <td>0.011342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0v8icS8wOOgEDiHDCOQkZQ</td>\n",
       "      <td>Chilli Chicken House</td>\n",
       "      <td>4040 Creditview Road, Unit 25</td>\n",
       "      <td>L5C</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>4.0</td>\n",
       "      <td>320</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.028474</td>\n",
       "      <td>0.065532</td>\n",
       "      <td>0.863240</td>\n",
       "      <td>0.013083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2Q89PhkcyT9uZnQORmqMKg</td>\n",
       "      <td>House of Gourmet</td>\n",
       "      <td>484 Dundas St W</td>\n",
       "      <td>M5T 1G9</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>3.5</td>\n",
       "      <td>345</td>\n",
       "      <td>0.025629</td>\n",
       "      <td>0.059035</td>\n",
       "      <td>0.029460</td>\n",
       "      <td>0.836558</td>\n",
       "      <td>0.040546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>yY3jNsrpCyKTqQuRuLV8gw</td>\n",
       "      <td>Cactus Club Cafe</td>\n",
       "      <td>77 Adelaide Street W</td>\n",
       "      <td>M5H 1P9</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>4.0</td>\n",
       "      <td>393</td>\n",
       "      <td>0.016287</td>\n",
       "      <td>0.040831</td>\n",
       "      <td>0.035991</td>\n",
       "      <td>0.880841</td>\n",
       "      <td>0.019201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>yg_A_TpYkJjr1fef0J6QkQ</td>\n",
       "      <td>Konjiki Ramen</td>\n",
       "      <td>5051 Yonge Street</td>\n",
       "      <td>M2N 5P2</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>4.0</td>\n",
       "      <td>393</td>\n",
       "      <td>0.258226</td>\n",
       "      <td>0.132209</td>\n",
       "      <td>0.068830</td>\n",
       "      <td>0.524764</td>\n",
       "      <td>0.008854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>zA6gnF5aPBGoOm6uIbKt-A</td>\n",
       "      <td>Yuzu No Hana</td>\n",
       "      <td>236 Adelaide Street W</td>\n",
       "      <td>M5H 1W7</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>4.0</td>\n",
       "      <td>498</td>\n",
       "      <td>0.022650</td>\n",
       "      <td>0.235822</td>\n",
       "      <td>0.013813</td>\n",
       "      <td>0.710082</td>\n",
       "      <td>0.006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>zfQ855VX3SMA_54oVSN5Cw</td>\n",
       "      <td>Fresco's Fish &amp; Chips</td>\n",
       "      <td>201 Augusta Avenue</td>\n",
       "      <td>M5T 2L4</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>4.5</td>\n",
       "      <td>320</td>\n",
       "      <td>0.011593</td>\n",
       "      <td>0.021616</td>\n",
       "      <td>0.033088</td>\n",
       "      <td>0.883727</td>\n",
       "      <td>0.032494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>zgQHtqX0gqMw1nlBZl2VnQ</td>\n",
       "      <td>Momofuku Noodle Bar</td>\n",
       "      <td>190 University Avenue, Ground Floor</td>\n",
       "      <td>M5H 0A3</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>3.0</td>\n",
       "      <td>897</td>\n",
       "      <td>0.238388</td>\n",
       "      <td>0.177521</td>\n",
       "      <td>0.135544</td>\n",
       "      <td>0.409175</td>\n",
       "      <td>0.018646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id                   name  \\\n",
       "0    -av1lZI1JDY_RZN2eTMnWg  Salad King Restaurant   \n",
       "1    0J_NiF5Lb0bFM5v6ZY-uGw       Zet's Restaurant   \n",
       "2    0a2O150ytxrDjDzXNfRWkA                   Miku   \n",
       "3    0v8icS8wOOgEDiHDCOQkZQ   Chilli Chicken House   \n",
       "4    2Q89PhkcyT9uZnQORmqMKg       House of Gourmet   \n",
       "..                      ...                    ...   \n",
       "166  yY3jNsrpCyKTqQuRuLV8gw       Cactus Club Cafe   \n",
       "167  yg_A_TpYkJjr1fef0J6QkQ          Konjiki Ramen   \n",
       "168  zA6gnF5aPBGoOm6uIbKt-A           Yuzu No Hana   \n",
       "169  zfQ855VX3SMA_54oVSN5Cw  Fresco's Fish & Chips   \n",
       "170  zgQHtqX0gqMw1nlBZl2VnQ    Momofuku Noodle Bar   \n",
       "\n",
       "                                 address postal_code         city  stars_y  \\\n",
       "0                       340 Yonge Street     M5B 1R8      Toronto      3.5   \n",
       "1                      6445 Airport Road     L4V 1Y4  Mississauga      4.0   \n",
       "2                      105-10 Bay Street     M5J 2R8      Toronto      4.0   \n",
       "3          4040 Creditview Road, Unit 25         L5C  Mississauga      4.0   \n",
       "4                        484 Dundas St W     M5T 1G9      Toronto      3.5   \n",
       "..                                   ...         ...          ...      ...   \n",
       "166                 77 Adelaide Street W     M5H 1P9      Toronto      4.0   \n",
       "167                    5051 Yonge Street     M2N 5P2      Toronto      4.0   \n",
       "168                236 Adelaide Street W     M5H 1W7      Toronto      4.0   \n",
       "169                   201 Augusta Avenue     M5T 2L4      Toronto      4.5   \n",
       "170  190 University Avenue, Ground Floor     M5H 0A3      Toronto      3.0   \n",
       "\n",
       "     review_count         0         1         2         3         4  \n",
       "0             876  0.021768  0.025935  0.018437  0.910842  0.009212  \n",
       "1             333  0.027666  0.027748  0.052672  0.733037  0.111818  \n",
       "2             604  0.018702  0.343642  0.012161  0.606169  0.011342  \n",
       "3             320  0.014346  0.028474  0.065532  0.863240  0.013083  \n",
       "4             345  0.025629  0.059035  0.029460  0.836558  0.040546  \n",
       "..            ...       ...       ...       ...       ...       ...  \n",
       "166           393  0.016287  0.040831  0.035991  0.880841  0.019201  \n",
       "167           393  0.258226  0.132209  0.068830  0.524764  0.008854  \n",
       "168           498  0.022650  0.235822  0.013813  0.710082  0.006333  \n",
       "169           320  0.011593  0.021616  0.033088  0.883727  0.032494  \n",
       "170           897  0.238388  0.177521  0.135544  0.409175  0.018646  \n",
       "\n",
       "[171 rows x 12 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 90.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Renaming columns\n",
    "\n",
    "grp_mean_pos.rename(columns= pos_dict, inplace=True)\n",
    "grp_mean_avg.rename(columns= avg_dict, inplace=True)\n",
    "grp_mean_neg.rename(columns= neg_dict, inplace=True)\n",
    "\n",
    "\n",
    "final_pos.rename(columns= pos_dict, inplace=True)\n",
    "final_avg.rename(columns= avg_dict, inplace=True)\n",
    "final_neg.rename(columns= neg_dict, inplace=True)\n",
    "\n",
    "\n",
    "grp_mean_pos.to_csv('modelled_positive.csv')\n",
    "grp_mean_avg.to_csv('modelled_average.csv')\n",
    "grp_mean_neg.to_csv('modelled_negative.csv')\n",
    "final_pos.to_csv('modelled_positive_full.csv')\n",
    "final_avg.to_csv('modelled_average_full.csv')\n",
    "final_neg.to_csv('modelled_negative_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
